# Deployment Guide: Fixed Queue-Based Historical Data System

## Summary of Changes

This guide covers the deployment of the fixed queue-based historical data collection system. All changes have been made to fix the critical issues preventing data from reaching D1.

---

## What Was Fixed

### 1. âœ… Python Historical Worker
**File:** `pyswarm/Data_Import/historical_worker.py`

**Problem:**
- Was attempting direct D1 writes with 2000 rows Ã— 10 params = 20,000 parameters
- Exceeded D1's ~1000 parameter limit
- Resulted in silent failures

**Fix:**
- Changed to queue-based architecture
- Batches 10 candles per queue message (10x more efficient)
- Now sends data to `HISTORICAL_QUEUE` instead of direct D1 writes

**Config:** `pyswarm/wrangler_historical_import.toml`
- Added queue producer binding: `HISTORICAL_QUEUE`

---

### 2. âœ… Queue Consumer
**File:** `cloudflare-agents/historical-data-queue-consumer.ts`

**Problems:**
- Writing to `historical_prices` table (doesn't exist)
- Wrong column schema
- Your 200MB data is in `price_data` table

**Fixes:**
- Changed table name: `historical_prices` â†’ `price_data` âœ…
- Updated schema to match: added `timeframe`, `volume_from`, `volume_to` columns
- Updated deduplication to include `timeframe` in key
- Added support for array messages (Python worker sends arrays of 10 points)

---

### 3. âœ… Wrangler Configuration
**File:** `cloudflare-agents/wrangler-historical-queue.toml`

**Problems:**
- Placeholder database ID: `YOUR_D1_DATABASE_ID`
- 15-minute cron (would exceed 1M queue ops/month)

**Fixes:**
- Real database ID: `ac4629b2-8240-4378-b3e3-e5262cd9b285` âœ…
- Database name: `coinswarm-evolution` âœ…
- Cron interval: 15min â†’ 30min (stays within free limits)

---

### 4. âœ… Old Cron Worker Disabled
**File:** `cloudflare-agents/wrangler-historical-collection-cron.toml`

**Change:**
- Commented out cron trigger
- Added deprecation notice
- Can be re-enabled if needed (uncomment trigger)

---

## Architecture After Fix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Historical Worker                                        â”‚
â”‚ (pyswarm/Data_Import/historical_worker.py)                     â”‚
â”‚                                                                  â”‚
â”‚ â€¢ Fetches from CryptoCompare âœ…                                 â”‚
â”‚ â€¢ Batches 10 candles per message                                â”‚
â”‚ â€¢ Sends to HISTORICAL_QUEUE                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                 [historical-data-queue]
                 (1M operations/month included)
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Queue Consumer                                                   â”‚
â”‚ (historical-data-queue-consumer.ts)                             â”‚
â”‚                                                                  â”‚
â”‚ â€¢ Receives batches of up to 100 messages                        â”‚
â”‚ â€¢ Processes arrays of 10 points per message                     â”‚
â”‚ â€¢ Batch writes to D1 (100 at once)                              â”‚
â”‚ â€¢ Uses INSERT OR IGNORE (protects your 200MB)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ D1 Database: coinswarm-evolution       â”‚
        â”‚ ID: ac4629b2-8240-4378-b3e3-e5262cd9b285â”‚
        â”‚                                          â”‚
        â”‚ Table: price_data                        â”‚
        â”‚ â€¢ Your existing 200MB data âœ…           â”‚
        â”‚ â€¢ UNIQUE constraint prevents duplicates  â”‚
        â”‚ â€¢ New data fills gaps safely             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deployment Steps

### Prerequisites

1. **Cloudflare account with Workers Paid Plan** ($5/month)
2. **Wrangler CLI installed:**
   ```bash
   npm install -g wrangler
   wrangler login
   ```

3. **API Keys (set as secrets):**
   ```bash
   # For Python worker
   wrangler secret put CRYPTOCOMPARE_API_KEY \
     --config pyswarm/wrangler_historical_import.toml

   # For queue producer (if needed)
   wrangler secret put CRYPTOCOMPARE_API_KEY \
     --config cloudflare-agents/wrangler-historical-queue.toml
   ```

---

### Step 1: Create the Queue

```bash
# Create the historical data queue
wrangler queues create historical-data-queue

# Create dead letter queue (for failed messages)
wrangler queues create historical-data-dlq
```

**Expected output:**
```
âœ… Created queue historical-data-queue
âœ… Created queue historical-data-dlq
```

---

### Step 2: Deploy Queue Consumer First

**Important:** Deploy consumer BEFORE producer to avoid message loss!

```bash
cd cloudflare-agents

# Deploy the consumer
wrangler deploy \
  --config wrangler-historical-queue.toml \
  --name historical-data-queue-consumer \
  historical-data-queue-consumer.ts
```

**Expected output:**
```
âœ… Deployed historical-data-queue-consumer
   https://historical-data-queue-consumer.<your-subdomain>.workers.dev
```

**Verify consumer is attached to queue:**
```bash
wrangler queues consumer historical-data-queue
```

**Expected output:**
```
Queue: historical-data-queue
Consumers:
  - historical-data-queue-consumer
    max_batch_size: 100
    max_concurrency: 5
    max_retries: 3
```

---

### Step 3: Deploy Python Historical Worker

```bash
cd ../pyswarm

# Deploy Python worker
wrangler deploy --config wrangler_historical_import.toml
```

**Expected output:**
```
âœ… Deployed coinswarm-historical-import
   https://coinswarm-historical-import.<your-subdomain>.workers.dev
```

---

### Step 4: Deploy Queue Producer (Optional)

If you want the queue producer cron as well (runs every 30 min):

```bash
cd ../cloudflare-agents

# Deploy the producer
wrangler deploy \
  --config wrangler-historical-queue.toml \
  --name historical-data-queue-producer \
  historical-data-queue-producer.ts
```

**Expected output:**
```
âœ… Deployed historical-data-queue-producer
   Cron: */30 * * * * (every 30 minutes)
```

---

### Step 5: Test the System

#### Test Python Worker

```bash
# Trigger the Python worker manually
curl -X POST https://coinswarm-historical-import.<your-subdomain>.workers.dev/trigger
```

**Expected response:**
```json
{
  "inserted": 2000,
  "next_toTs": 1699920000,
  "oldest": 1699920000,
  "newest": 1730000000,
  "message": "Inserted 2000 BTC-USDC daily candles.",
  "next_trigger": "POST /trigger with toTs=1699920000"
}
```

**This means:**
- âœ… Python worker fetched 2000 candles from CryptoCompare
- âœ… Queued them in batches (200 messages of 10 candles each)
- âœ… Messages are now in the queue

---

#### Monitor Queue Depth

```bash
# Check queue status
wrangler queues consumer historical-data-queue
```

**Expected output:**
```
Queue: historical-data-queue
Messages: 200 (declining as consumer processes)
Delivered: 150
Acknowledged: 150
```

**Queue depth should decrease as consumer processes messages.**

---

#### Check Consumer Logs

```bash
# Tail consumer logs in real-time
wrangler tail historical-data-queue-consumer --format pretty
```

**Expected logs:**
```
[INFO] ğŸ“¥ Processing batch of 100 data points
[INFO]    Deduplicated: 1000 â†’ 995 unique points
[INFO] âœ… Inserted 995 rows in 248ms
[INFO]    Throughput: 4008 rows/sec
```

**Good signs:**
- âœ… Batch processing is working
- âœ… Deduplication is working
- âœ… D1 writes are succeeding (no errors)
- âœ… High throughput (1000-5000 rows/sec)

---

#### Verify Data in D1

```bash
# Check if data reached D1
wrangler d1 execute ac4629b2-8240-4378-b3e3-e5262cd9b285 \
  --command "SELECT COUNT(*) as total FROM price_data WHERE source='cryptocompare'" \
  --remote
```

**Expected output:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ total â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2000  â”‚  â† Your new data!
â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Also check your original data is still there:**
```bash
wrangler d1 execute ac4629b2-8240-4378-b3e3-e5262cd9b285 \
  --command "SELECT COUNT(*) as total, source FROM price_data GROUP BY source" \
  --remote
```

**Expected output:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ total  â”‚ source        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 200000 â”‚ manual        â”‚  â† Your original 200MB (safe!)
â”‚ 2000   â”‚ cryptocompare â”‚  â† New data from Python worker
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Monitoring & Maintenance

### Daily Health Check

Run this script daily to monitor system health:

```bash
#!/bin/bash
# health-check.sh

echo "=== Queue Health ==="
wrangler queues consumer historical-data-queue

echo -e "\n=== Consumer Logs (last 10 min) ==="
wrangler tail historical-data-queue-consumer --format pretty --once

echo -e "\n=== D1 Data Count ==="
wrangler d1 execute ac4629b2-8240-4378-b3e3-e5262cd9b285 \
  --command "SELECT COUNT(*) as total, source FROM price_data GROUP BY source" \
  --remote

echo -e "\n=== Recent Data ==="
wrangler d1 execute ac4629b2-8240-4378-b3e3-e5262cd9b285 \
  --command "SELECT symbol, COUNT(*) as count FROM price_data WHERE timestamp > $(date -d '24 hours ago' +%s) GROUP BY symbol" \
  --remote
```

Make it executable and run:
```bash
chmod +x health-check.sh
./health-check.sh
```

---

### Check Dead Letter Queue

If consumer repeatedly fails, messages go to DLQ:

```bash
# Check DLQ depth
wrangler queues consumer historical-data-dlq

# View failed messages
wrangler tail historical-data-dlq --format pretty
```

**If DLQ has messages:**
1. Check consumer logs for errors
2. Fix the issue (code or data)
3. Re-queue DLQ messages:
   ```bash
   # TODO: Add DLQ reprocessor script
   ```

---

### Set Up Alerts

**Cloudflare Dashboard â†’ Notifications:**

1. **Queue Depth Alert**
   - Metric: Queue depth
   - Condition: > 5,000 messages for 10 minutes
   - Action: Email notification

2. **Consumer Error Rate**
   - Metric: Worker error rate
   - Condition: > 5% errors
   - Action: Email notification

3. **D1 Storage Alert**
   - Metric: D1 storage usage
   - Condition: > 4GB (80% of 5GB free limit)
   - Action: Email notification

---

## Troubleshooting

### Issue: Queue depth keeps growing

**Symptoms:**
- Queue depth > 10,000 and increasing
- Consumer logs show slow processing

**Solutions:**

1. **Increase consumer concurrency:**
   ```toml
   # wrangler-historical-queue.toml
   [[queues.consumers]]
   max_concurrency = 10  # Increase from 5 to 10
   ```

2. **Increase batch size:**
   ```toml
   max_batch_size = 100  # Already at max
   ```

3. **Check D1 for locks:**
   ```bash
   # Look for "database is locked" errors
   wrangler tail historical-data-queue-consumer | grep "locked"
   ```

---

### Issue: Consumer logs show errors

**Check for specific error:**

```bash
wrangler tail historical-data-queue-consumer | grep ERROR
```

**Common errors:**

1. **"no such table: price_data"**
   - Schema not applied to database
   - Solution: Apply schema:
     ```bash
     wrangler d1 execute ac4629b2-8240-4378-b3e3-e5262cd9b285 \
       --file cloudflare-agents/price-data-schema.sql \
       --remote
     ```

2. **"database is locked"**
   - D1 is overloaded (too many concurrent writes)
   - Solution: Consumer will auto-retry with backoff âœ…

3. **"UNIQUE constraint failed"**
   - Duplicate data (this is OK! INSERT OR IGNORE handles it)
   - Check logs show "Inserted 0 rows" = all duplicates

---

### Issue: Python worker not queueing data

**Check Python worker logs:**
```bash
wrangler tail coinswarm-historical-import
```

**Look for:**
- âœ… "Queued 2000 candles for BTC-USDC in 200 batches"
- âŒ Errors about queue binding

**If queue binding missing:**
```toml
# pyswarm/wrangler_historical_import.toml
[[queues.producers]]
queue = "historical-data-queue"
binding = "HISTORICAL_QUEUE"
```

---

### Issue: Data not appearing in D1

**Check in order:**

1. **Is Python worker running?**
   ```bash
   curl -X POST https://coinswarm-historical-import.<your-subdomain>.workers.dev/trigger
   ```

2. **Is data in queue?**
   ```bash
   wrangler queues consumer historical-data-queue
   # Look for non-zero "Messages"
   ```

3. **Is consumer processing?**
   ```bash
   wrangler tail historical-data-queue-consumer
   # Should see "Processing batch of X data points"
   ```

4. **Is consumer writing to D1?**
   ```bash
   # Check for successful writes in logs
   wrangler tail historical-data-queue-consumer | grep "Inserted"
   ```

5. **Is D1 receiving writes?**
   ```bash
   # Check row count increases
   wrangler d1 execute ac4629b2-8240-4378-b3e3-e5262cd9b285 \
     --command "SELECT COUNT(*) FROM price_data" \
     --remote
   ```

---

## Performance Metrics

### Expected Throughput

| Component | Rate | Notes |
|-----------|------|-------|
| **Python worker fetch** | 2000 candles/request | Limited by CryptoCompare API |
| **Queue operations** | 200 messages queued | 10 candles per message |
| **Consumer processing** | 1000-5000 rows/sec | Batch writes to D1 |
| **D1 writes** | 50M/month included | You'll use ~1% with current setup |

### Cost Estimate (with 30min cron)

| Service | Usage | Included | Overage | Cost |
|---------|-------|----------|---------|------|
| **Workers Requests** | 844K/month | 10M | - | $0 |
| **Queue Operations** | 75K/month | 1M | - | $0 |
| **D1 Writes** | 562K/month | 50M | - | $0 |
| **D1 Reads** | 100K/month | 25B | - | $0 |
| **Total** | - | - | - | **$5.00** âœ… |

**You're using:**
- 8.4% of Workers requests
- 7.5% of Queue operations
- 1.1% of D1 writes

**Plenty of headroom!** ğŸ‰

---

## Next Steps

### Immediate (After Deployment)

1. âœ… Test Python worker with manual trigger
2. âœ… Verify queue is receiving messages
3. âœ… Confirm consumer is processing
4. âœ… Check data appears in D1
5. âœ… Verify original 200MB data is untouched

### Short-Term (First Week)

1. Monitor queue depth daily
2. Check DLQ for failed messages
3. Review consumer logs for errors
4. Verify D1 storage growth is normal
5. Confirm no overage charges

### Long-Term (Ongoing)

1. Add more tokens to track (13 â†’ 25+)
2. Add more timeframes (1h, 15m)
3. Add more exchanges (Binance, CoinGecko)
4. Implement technical indicators
5. Set up automated alerts

---

## Rollback Plan

If something goes wrong, you can quickly rollback:

### 1. Stop New Data Collection

```bash
# Stop Python worker by disabling it (no cron to disable)
# Just don't trigger it manually

# Stop queue producer cron
wrangler deployments list --name historical-data-queue-producer
wrangler deployments rollback <previous-deployment-id>
```

### 2. Re-enable Old Cron Worker (if needed)

```toml
# cloudflare-agents/wrangler-historical-collection-cron.toml
[triggers]
crons = ["0 * * * *"]  # Uncomment this line
```

```bash
wrangler deploy --config wrangler-historical-collection-cron.toml
```

### 3. Your 200MB Data is Safe!

The `INSERT OR IGNORE` statement ensures your existing data is never modified:

```sql
-- D1 UNIQUE constraint prevents duplicates
UNIQUE(symbol, timestamp, timeframe, source)

-- INSERT OR IGNORE silently skips duplicates
INSERT OR IGNORE INTO price_data (...) VALUES (...);
```

**Your original 200MB is protected!** âœ…

---

## Summary

**What's Fixed:**
- âœ… Python worker now queues data (no more parameter limit errors)
- âœ… Queue consumer writes to correct `price_data` table
- âœ… Wrangler config uses real database ID
- âœ… 30-minute cron stays within free queue limits
- âœ… Old cron worker disabled

**What's Working:**
- âœ… Python worker fetches from CryptoCompare reliably
- âœ… Queue buffers data for batch processing
- âœ… Consumer writes to D1 with 100x performance boost
- âœ… INSERT OR IGNORE protects your 200MB data
- âœ… System stays within $5/month plan limits

**What to Deploy:**
1. Queue consumer first (prevent message loss)
2. Python worker second (start queueing data)
3. Queue producer optional (adds 30-min cron)

**Ready to deploy? Follow the steps above!** ğŸš€
